{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5week.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTeBoXjVd-Rg"
      },
      "source": [
        "## 5-2 여러 웹 페이지에서 이미지 다운로드 받기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en6qayGasmTv"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "urls = [\"https://news.v.daum.net/v/20210926122646652\",\n",
        "       \"https://news.v.daum.net/v/20210926120023183\",\n",
        "       \"https://news.v.daum.net/v/20210926103307509\"]\n",
        "\n",
        "for url in urls:\n",
        "  res = requests.get(url)\n",
        "  html_text = res.text\n",
        "  soup = bs4.BeautifulSoup(html_text, \"html.parser\")\n",
        "  tag_list = soup.select(\"section > figure img\")\n",
        "  for img in tag_list :\n",
        "    src = img[\"src\"]\n",
        "    filename = os.path.basename(src)\n",
        "    urllib.request.urlretrieve(src, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thcPlam2d-aD"
      },
      "source": [
        "## 5-3 크롤링 결과 리스트 객체와 텍스트 파일에 저장하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkTMnZ8BjDKR"
      },
      "source": [
        "**1. 필요한 데이터 리스트로 객체화하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAc31ljJeLhE"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "import os\n",
        "\n",
        "url = \"https://news.v.daum.net/v/20210921190103948\"\n",
        "res = requests.get(url)\n",
        "html_text = res.text\n",
        "soup = bs4.BeautifulSoup(html_text, \"html.parser\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-RfXFEreM4E"
      },
      "source": [
        "tag = soup.select_one(\"h3.tit_view\")\n",
        "tag_text = tag.text #tag안의 text내용을 담기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j29hiwQjfBrk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980799fb-358d-46d2-f33f-9aa5c9bcb818"
      },
      "source": [
        "# 빈 리스트 만들기\n",
        "dataAll = []\n",
        "dataAll.append(tag_text) #리스트 요소 추가 : .append() 괄호 안에 요소 및 변수 넣기\n",
        "print(dataAll)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['완전식품 달걀보다 더 풍부한 단백질 음식 6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTagbgsofuuF",
        "outputId": "00219a1a-f308-4fe9-bac2-341d01ae4024"
      },
      "source": [
        "# 나머지 내용 dataAll에 넣는 for문 만들기\n",
        "\n",
        "tag_list = soup.select('section > p')\n",
        "\n",
        "for x in tag_list:\n",
        "  tag_text = x.text\n",
        "  dataAll.append(tag_text)\n",
        "\n",
        "print(dataAll)\n",
        "print(len(dataAll))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['완전식품 달걀보다 더 풍부한 단백질 음식 6', '단백질은 우리 몸에 꼭 필요한 영양소 중 하나로 신체의 성장과 발달에 중요한 역할을 한다. 가장 잘 알려진 단백질 공급원은 우리가 쉽게 접할 수 있는 달걀인데, 달걀 한 알에는 약 6g의 단백질이 들어있다. 하지만 달걀 외에도 그 이상으로 단백질을 섭취할 수 있는 식품이 여럿 있다. ', '미국 온라인 건강정보 포털 웸엡디(WebMD)에 소개된 단백질이 풍부한 음식을 알아보자. ', '1. 병아리콩 = 병아리콩 반 컵에는 단백질이 약 8g 들어있다. 이집트의 대중음식인 후무스를 만드는 베이스로 사용되는 병아리콩은 고대 이집트 때부터 사람들이 즐겨먹던 음식이다. 샐러드에 한 줌 넣어먹거나 수프로 만들어 먹을 수 있다. ', '2. 코티지 치즈 = 코티지 치즈 반 컵에는 약 12g의 단백질이 들어있다. 코티지 치즈 자체의 맛이 강하지 않기 때문에 어떤 음식과도 잘 어울린다. 과일과 코티지 치즈를 함께 곁들여 먹으면 건강한 간식이 된다. 건강을 위해서 저지방 코티지 치즈를 먹도록 하자. ', '3. 렌틸콩 = 렌틸콩 반 컵으로 약 8g의 단백질을 얻을 수 있다. 이는 살코기 스테이크 약 30g으로 섭취할 수 있는 단백질 양과 맞먹는다. 보통 콩보다 빨리 익고 불릴 필요가 없어 요리하기도 편하다. 여러 가지 색의 렌틸콩을 이용해 다양한 요리를 즐길 수 있다. 갈색은 야채버거에, 초록색은 샐러드에, 빨간색은 매운 커리에 활용해보라. ', '4. 호박씨 = 껍질을 벗긴 호박씨 약 30g에는 8.5g가량의 단백질이 들어있다. 호박씨는 아연, 철분, 마그네슘, 칼륨, 셀레늄 등 다른 영양소도 함유하고 있다. 호박씨 한 줌을 사과와 함께 곁들여 먹으면 포만감이 있는 간식이 될 수 있다. 오트밀이나 그래놀라 등에 넣어 먹어도 좋다. ', '5. 새우 = 새우 약 110g 정도에 17g이 넘는 단백질이 들어있다. 칼로리와 지방이 낮고 수은도 거의 들어있지 않다. 빵가루를 묻혀 튀긴 것보다 구워 먹는 것이 좋다. ', '6. 육포 = 육포는 쇠고기를 얇게 저미어 말린 포이다. 육포 약 30g으로 최대 15g의 단백질을 섭취할 수 있다. 시중에 나온 육포의 경우 소금이나 설탕, 질산염과 같은 첨가물이 많이 들어있다. 식품 라벨을 확인해 나트륨, 설탕, 화학물질 등이 들어있지 않은지 체크하라. 직접 만들어 먹을 수도 있다. 요즘에는 쇠고기 외에 칠면조나 연어 등을 사용한 육포도 많다.', '정희은 기자 (eun@kormedi.com)']\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyUgNutoi_L8"
      },
      "source": [
        "**2. 리스트를 .txt파일로 저장하기 (p.55)**\n",
        "\n",
        "- with open(\"file경로\", 모드, encoding=\"cp949\" 또는 \"utf8\") as file객체 :\n",
        "      \n",
        "  file객체.write(\"저장할 문자 데이터\") #file 객체에서 문자열 추출\n",
        "\n",
        "- file객체 = open(\"file경로\", 모드, \"encoding=\"cp949\" 또는 \"utf8\")\n",
        "\n",
        "  file객체.write(\"저장할 문자데이터\")\n",
        "\n",
        "  f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-XJrVzZgfzQ"
      },
      "source": [
        "tag_list = soup.select('section > p')\n",
        "\n",
        "for x in tag_list:\n",
        "  tag_text = x.text\n",
        "  dataAll.append(tag_text)\n",
        "\n",
        "\n",
        "for x in dataAll:\n",
        "  #at = 데이터 덮어쓰지 않고 추가해서 넣기 / wt로 할 경우, 맨 마지막 list[-1]내용만 남을 것임\n",
        "  with open(\"[20210929]data.txt\",\"at\",encoding=\"utf-8\") as f: \n",
        "    f.write(x) #dataAll 내용 하나씩 넣기\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mzj3ODZNnfwe"
      },
      "source": [
        "**3. 기자 정보(byline)이 필요없는 경우**\n",
        "\n",
        "- 마지막 p요소 삭제하고 싶은 경우\n",
        " 1. del dataAll[-1]을 사용하여 마지막 '정희은 기자' 부분 삭제하기\n",
        " 2. dataAll.pop() 명령어도 가능"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPT4bfoooMkQ"
      },
      "source": [
        "**4. 다른 예제로 크롤링 해보기**\n",
        "\n",
        "- url : news.v.daum.net/v/20210926122646652\n",
        "- 내용을 크롤링 하고 싶은데, div와 p태그에 내용이 섞여있음\n",
        "- 그룹 선택자 (group selector)가 필요 (태그들은 콤마(,)를 이용한다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmqNsABdnKmS"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "import os\n",
        "\n",
        "url = 'http://news.v.daum.net/v/20210926122646652'\n",
        "res = requests.get(url)\n",
        "html_text = res.text\n",
        "soup = bs4.BeautifulSoup(html_text,'html.parser')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVtehLXXpCUb"
      },
      "source": [
        "tag_list = soup.select(\"section > p, section > div\") #띄어쓰기 해주기 필수\n",
        "dataAll = []\n",
        "for x in tag_list:\n",
        "  tag_text = x.text\n",
        "  dataAll.append(tag_text)\n",
        "\n",
        "del dataAll[-1] #by line 삭제\n",
        "\n",
        "for x in dataAll:\n",
        "  with open('20210929-20210926122646652','at',encoding='utf-8') as f:\n",
        "    f.write(x)\n",
        "    f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMXVJOktqcaS"
      },
      "source": [
        "**5. 여러개 뉴스 기사 내용 크롤링하기**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ntxhJRpV7W"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "import os\n",
        "\n",
        "urllist = ['http://news.v.daum.net/v/20210926122646652','https://news.v.daum.net/v/20210921190103948','https://news.v.daum.net/v/20210926103307509']\n",
        "\n",
        "def url(x):\n",
        "  res = requests.get(x)\n",
        "  html_text = res.text\n",
        "  soup = bs4.BeautifulSoup(html_text,'html.parser')\n",
        "\n",
        "  tag_list = soup.select(\"section > p, section > div\") #띄어쓰기 해주기 필수\n",
        "  dataAll = []\n",
        "  for x in tag_list:\n",
        "    tag_text = x.text\n",
        "    dataAll.append(tag_text)\n",
        "\n",
        "  del dataAll[-1] #by line 삭제\n",
        "  dataAll.append('\\n\\n')\n",
        "  for x in dataAll:\n",
        "    with open('news_textdata','at',encoding='utf-8') as f:\n",
        "      f.write(x)\n",
        "      f.close()\n",
        "\n",
        "\n",
        "for x in urllist:\n",
        "  url(x)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zdu5kGKVRFy"
      },
      "source": [
        "### 과제 \n",
        "\n",
        "- 세 개의 웹페이지 url을 포함하는 리스트 객체 ulrs가 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXWcNgD-UQmK"
      },
      "source": [
        "import requests\n",
        "import bs4\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "urls = [\"https://news.v.daum.net/v/20210928100600060\",\n",
        "       \"https://news.v.daum.net/v/20210927220302143\",\n",
        "       \"https://news.v.daum.net/v/20210928150100446\"]\n",
        "\n",
        "def url(x):\n",
        "  res = requests.get(x)\n",
        "  html_text = res.text\n",
        "  soup = bs4.BeautifulSoup(html_text,'html.parser')\n",
        " \n",
        "\n",
        "  dataAll = []\n",
        "  # 뉴스기사 타이틀\n",
        "  title = soup.select_one(\"h3.tit_view\")\n",
        "  h3 = title.text\n",
        "  dataAll.append(h3)\n",
        "\n",
        "  # 뉴스기사 내용, 본문\n",
        "  tag_list = soup.select(\"section > p, section > div\") #띄어쓰기 해주기 필수\n",
        "  \n",
        "  for x in tag_list:\n",
        "    tag_text = x.text\n",
        "    dataAll.append(tag_text)\n",
        "\n",
        "  del dataAll[-1] #by line 삭제\n",
        "  dataAll.append('\\n\\n') #줄바꿈 두 번\n",
        "  for x in dataAll:\n",
        "    with open('news_data.txt','at',encoding='utf-8') as f: #인코딩 'utf-8's\n",
        "      f.write(x)\n",
        "      f.close()\n",
        "\n",
        "\n",
        "for x in urls:\n",
        "  url(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfPsR-bZWC2z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}